{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99b3164d-c5b5-4768-af8b-9123036f4b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#geral\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "#visual\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pprint import pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5799cd10-8d12-41b0-92ce-b6ec0c8f6eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/all_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "296cc47d-8fa7-438f-83eb-ee904636ea65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1999515, 21)\n"
     ]
    }
   ],
   "source": [
    "df.dropna(subset=['comment_text', 'toxicity'], inplace=True)\n",
    "df.dropna(axis='columns', inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e2543d0-e6f5-4f17-ad51-f726d3c6e229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1971915, 21)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove duplicados\n",
    "df.drop_duplicates(subset=['comment_text',], keep='first', inplace=True)\n",
    "df.shape # 1999515 - 1971915 = 27600 duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "054d772a-ca5e-4265-a8c0-9d15a79e6f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove os caracteres \\xad que aparecem em alguns spams\n",
    "df.comment_text = df.comment_text.replace('\\xad', '', regex=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c28bb013-a003-4f4d-bc21-367a7cea2105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand contractions\n",
    "import contractions\n",
    "df.comment_text = df.comment_text.apply(contractions.fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9aea6e8b-d70a-4812-8d56-c46976f4b201",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = np.where(df['toxicity'] >= 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d259e6ed-d49a-44f9-84f6-b170e5a9da5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['comment_text', 'label'],\n",
       "        num_rows: 283813\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['comment_text', 'label'],\n",
       "        num_rows: 31535\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X = df.comment_text.to_numpy().reshape(-1,1)\n",
    "y = np.where(df[['toxicity']] >= 0.5, 1, 0).reshape(-1,1)\n",
    "\n",
    "under_sampler = RandomUnderSampler(random_state=0)\n",
    "X, y = under_sampler.fit_resample(X, y)\n",
    "\n",
    "raw_datasets = Dataset.from_dict({\n",
    "    'comment_text': X.ravel(),\n",
    "    'label': y,\n",
    "}).train_test_split(train_size=0.9, test_size=0.1)\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1febb1c9-596a-438e-8e44-0873be8fc082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/rafael/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 1-grams\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "tknzr = TweetTokenizer(preserve_case=False, reduce_len=False)\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1), tokenizer=tknzr.tokenize, strip_accents='unicode', lowercase=True, stop_words=stopwords.words('english'))\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "])\n",
    "features = pipeline.fit_transform(raw_datasets['train']['comment_text'], raw_datasets['train']['label'])\n",
    "feature_names = pipeline['vectorizer'].get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1bc602e-9a29-40b2-a7cd-48f1d2aa6cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\\x13', '!', '\"', '#', '###ing', '###konki', '###off', '##ed',\n",
       "       '##hole'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eab8af32-7bd9-4c82-a4fd-82dcf692df79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(features, raw_datasets['train']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bfaa840-46fe-4b84-8364-94c2ac83690d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/.cache/pypoetry/virtualenvs/mineracao-XH4Rs9qH-py3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1208: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31535,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7905501823370857, 'f1': 0.802782837180138}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "preds = clf.predict(pipeline.transform(raw_datasets[\"test\"][\"comment_text\"]))\n",
    "print(preds.shape)\n",
    "\n",
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "metric.compute(predictions=preds, references=raw_datasets[\"test\"][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57667abd-26f5-4d15-aeea-8448e3021126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11487,  4200],\n",
       "       [ 2405, 13443]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(raw_datasets[\"test\"][\"label\"], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eae49547-09ab-4dfa-ac6b-7027b8ac9a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': {'precision': 0.8268787791534696,\n",
      "       'recall': 0.7322623828647925,\n",
      "       'f1-score': 0.776699685587748,\n",
      "       'support': 15687},\n",
      " '1': {'precision': 0.7619452474069036,\n",
      "       'recall': 0.8482458354366481,\n",
      "       'f1-score': 0.802782837180138,\n",
      "       'support': 15848},\n",
      " 'accuracy': 0.7905501823370857,\n",
      " 'macro avg': {'precision': 0.7944120132801866,\n",
      "               'recall': 0.7902541091507203,\n",
      "               'f1-score': 0.789741261383943,\n",
      "               'support': 31535},\n",
      " 'weighted avg': {'precision': 0.7942462562069158,\n",
      "                  'recall': 0.7905501823370857,\n",
      "                  'f1-score': 0.7898078443458325,\n",
      "                  'support': 31535}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pp\n",
    "\n",
    "pp(classification_report(raw_datasets[\"test\"][\"label\"], preds, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3151f802-803a-4693-a0ec-ff1083a57b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/.cache/pypoetry/virtualenvs/mineracao-XH4Rs9qH-py3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1208: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8103060091961313, 'f1': 0.8052607591640081}\n",
      "array([[13185,  2502],\n",
      "       [ 3480, 12368]])\n",
      "{'0': {'precision': 0.7911791179117912,\n",
      "       'recall': 0.840504876649455,\n",
      "       'f1-score': 0.8150964391691394,\n",
      "       'support': 15687},\n",
      " '1': {'precision': 0.8317417619367855,\n",
      "       'recall': 0.7804139323573952,\n",
      "       'f1-score': 0.8052607591640081,\n",
      "       'support': 15848},\n",
      " 'accuracy': 0.8103060091961313,\n",
      " 'macro avg': {'precision': 0.8114604399242884,\n",
      "               'recall': 0.810459404503425,\n",
      "               'f1-score': 0.8101785991665738,\n",
      "               'support': 31535},\n",
      " 'weighted avg': {'precision': 0.8115639849645297,\n",
      "                  'recall': 0.8103060091961313,\n",
      "                  'f1-score': 0.8101534914373708,\n",
      "                  'support': 31535}}\n"
     ]
    }
   ],
   "source": [
    "# Ngrams 1-4 - kbest\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,4), tokenizer=tknzr.tokenize, strip_accents='unicode', lowercase=True, stop_words=stopwords.words('english'))\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('kbest', SelectKBest(k=1000)),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipeline.fit(raw_datasets['train']['comment_text'], raw_datasets['train']['label'])\n",
    "\n",
    "\n",
    "preds = pipeline.predict(raw_datasets[\"test\"][\"comment_text\"])\n",
    "\n",
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "pp(metric.compute(predictions=preds, references=raw_datasets[\"test\"][\"label\"]))\n",
    "pp(confusion_matrix(raw_datasets[\"test\"][\"label\"], preds))\n",
    "pp(classification_report(raw_datasets[\"test\"][\"label\"], preds, output_dict=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92f30a25-ae83-4a82-80d7-c4bef3fb9bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# função que remove caracteres não alfabéticos\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "def remove_non_alpha(vec):\n",
    "    for x in vec:\n",
    "        x = x.split()\n",
    "        yield ' '.join([s for s in x if s.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46d84e94-a965-4fca-a73a-c7f7b8db39b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.7546218487394958, 'f1': 0.7620102109860367}\n",
      "array([[11409,  4278],\n",
      "       [ 3460, 12388]])\n",
      "{'0': {'precision': 0.7673010962405004,\n",
      "       'recall': 0.7272901128322815,\n",
      "       'f1-score': 0.7467600471265872,\n",
      "       'support': 15687},\n",
      " '1': {'precision': 0.7433097323892955,\n",
      "       'recall': 0.781675921251893,\n",
      "       'f1-score': 0.7620102109860367,\n",
      "       'support': 15848},\n",
      " 'accuracy': 0.7546218487394958,\n",
      " 'macro avg': {'precision': 0.755305414314898,\n",
      "               'recall': 0.7544830170420873,\n",
      "               'f1-score': 0.7543851290563119,\n",
      "               'support': 31535},\n",
      " 'weighted avg': {'precision': 0.7552441710997395,\n",
      "                  'recall': 0.7546218487394958,\n",
      "                  'f1-score': 0.7544240584424127,\n",
      "                  'support': 31535}}\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes - 1-grams - Removing non alpha characters\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from datasets import load_metric\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from pprint import pp\n",
    "\n",
    "        \n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1), tokenizer=tknzr.tokenize, strip_accents='unicode', lowercase=True, stop_words=stopwords.words('english'))\n",
    "        \n",
    "        \n",
    "pipeline = Pipeline([\n",
    "    ('remove_non_alpha', FunctionTransformer(remove_non_alpha)),\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipeline.fit(raw_datasets['train']['comment_text'], raw_datasets['train']['label'])\n",
    "\n",
    "\n",
    "preds = pipeline.predict(raw_datasets[\"test\"][\"comment_text\"])\n",
    "\n",
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "pp(metric.compute(predictions=preds, references=raw_datasets[\"test\"][\"label\"]))\n",
    "pp(confusion_matrix(raw_datasets[\"test\"][\"label\"], preds))\n",
    "pp(classification_report(raw_datasets[\"test\"][\"label\"], preds, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11777e4f-379d-4875-a054-be0f62c0b67f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/.cache/pypoetry/virtualenvs/mineracao-XH4Rs9qH-py3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1208: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8101157444109719, 'f1': 0.8109848484848485}\n",
      "array([[12701,  2986],\n",
      "       [ 3002, 12846]])\n",
      "{'0': {'precision': 0.8088263389161306,\n",
      "       'recall': 0.8096513036272073,\n",
      "       'f1-score': 0.8092386110226185,\n",
      "       'support': 15687},\n",
      " '1': {'precision': 0.8113946437594745,\n",
      "       'recall': 0.810575466935891,\n",
      "       'f1-score': 0.8109848484848485,\n",
      "       'support': 15848},\n",
      " 'accuracy': 0.8101157444109719,\n",
      " 'macro avg': {'precision': 0.8101104913378026,\n",
      "               'recall': 0.8101133852815492,\n",
      "               'f1-score': 0.8101117297537335,\n",
      "               'support': 31535},\n",
      " 'weighted avg': {'precision': 0.8101170474988899,\n",
      "                  'recall': 0.8101157444109719,\n",
      "                  'f1-score': 0.8101161874076326,\n",
      "                  'support': 31535}}\n"
     ]
    }
   ],
   "source": [
    "# DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1), tokenizer=tknzr.tokenize, strip_accents='unicode', lowercase=True, stop_words=stopwords.words('english'))\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('clf', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "pipeline.fit(raw_datasets['train']['comment_text'], raw_datasets['train']['label'])\n",
    "\n",
    "\n",
    "preds = pipeline.predict(raw_datasets[\"test\"][\"comment_text\"])\n",
    "\n",
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "pp(metric.compute(predictions=preds, references=raw_datasets[\"test\"][\"label\"]))\n",
    "pp(confusion_matrix(raw_datasets[\"test\"][\"label\"], preds))\n",
    "pp(classification_report(raw_datasets[\"test\"][\"label\"], preds, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e61eacc9-27db-41d0-a9f9-1d5d3a057dd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.7054701125733313, 'f1': 0.7016957862281603}\n",
      "array([[11323,  4364],\n",
      "       [ 4924, 10924]])\n",
      "{'0': {'precision': 0.6969286637533083,\n",
      "       'recall': 0.7218078663861797,\n",
      "       'f1-score': 0.7091501221268868,\n",
      "       'support': 15687},\n",
      " '1': {'precision': 0.7145473574045003,\n",
      "       'recall': 0.6892983341746592,\n",
      "       'f1-score': 0.7016957862281603,\n",
      "       'support': 15848},\n",
      " 'accuracy': 0.7054701125733313,\n",
      " 'macro avg': {'precision': 0.7057380105789043,\n",
      "               'recall': 0.7055531002804194,\n",
      "               'f1-score': 0.7054229541775235,\n",
      "               'support': 31535},\n",
      " 'weighted avg': {'precision': 0.7057829861564822,\n",
      "                  'recall': 0.7054701125733313,\n",
      "                  'f1-score': 0.7054039253511449,\n",
      "                  'support': 31535}}\n"
     ]
    }
   ],
   "source": [
    "# DecisionTreeClassifier - remove_non_alpha\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1), tokenizer=tknzr.tokenize, strip_accents='unicode', lowercase=True, stop_words=stopwords.words('english'))\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('remove_non_alpha', FunctionTransformer(remove_non_alpha)),\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('clf', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "pipeline.fit(raw_datasets['train']['comment_text'], raw_datasets['train']['label'])\n",
    "\n",
    "\n",
    "preds = pipeline.predict(raw_datasets[\"test\"][\"comment_text\"])\n",
    "\n",
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "pp(metric.compute(predictions=preds, references=raw_datasets[\"test\"][\"label\"]))\n",
    "pp(confusion_matrix(raw_datasets[\"test\"][\"label\"], preds))\n",
    "pp(classification_report(raw_datasets[\"test\"][\"label\"], preds, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49fb8504-36af-4b46-a276-8d8b21339ae6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/.cache/pypoetry/virtualenvs/mineracao-XH4Rs9qH-py3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1208: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.7419375297288727, 'f1': 0.7501381639545593}\n",
      "array([[11181,  4506],\n",
      "       [ 3632, 12216]])\n",
      "{'0': {'precision': 0.7548099642206171,\n",
      "       'recall': 0.7127557850449416,\n",
      "       'f1-score': 0.7331803278688526,\n",
      "       'support': 15687},\n",
      " '1': {'precision': 0.7305346250448511,\n",
      "       'recall': 0.7708228167592125,\n",
      "       'f1-score': 0.7501381639545593,\n",
      "       'support': 15848},\n",
      " 'accuracy': 0.7419375297288727,\n",
      " 'macro avg': {'precision': 0.7426722946327341,\n",
      "               'recall': 0.7417893009020771,\n",
      "               'f1-score': 0.741659245911706,\n",
      "               'support': 31535},\n",
      " 'weighted avg': {'precision': 0.742610326508312,\n",
      "                  'recall': 0.7419375297288727,\n",
      "                  'f1-score': 0.7417025345054875,\n",
      "                  'support': 31535}}\n"
     ]
    }
   ],
   "source": [
    "# DecisionTreeClassifier 1-4-grams - kbest\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,4), tokenizer=tknzr.tokenize, strip_accents='unicode', lowercase=True, stop_words=stopwords.words('english'))\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('kbest', SelectKBest(k=1000)),\n",
    "    ('clf', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "pipeline.fit(raw_datasets['train']['comment_text'], raw_datasets['train']['label'])\n",
    "\n",
    "\n",
    "preds = pipeline.predict(raw_datasets[\"test\"][\"comment_text\"])\n",
    "\n",
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "pp(metric.compute(predictions=preds, references=raw_datasets[\"test\"][\"label\"]))\n",
    "pp(confusion_matrix(raw_datasets[\"test\"][\"label\"], preds))\n",
    "pp(classification_report(raw_datasets[\"test\"][\"label\"], preds, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ccb1492-fe4c-4fd6-a3fd-2d7eb2a7f1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/.cache/pypoetry/virtualenvs/mineracao-XH4Rs9qH-py3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1208: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8680196606944665, 'f1': 0.8642530984996738}\n",
      "array([[14124,  1563],\n",
      "       [ 2599, 13249]])\n",
      "{'0': {'precision': 0.8445853016803205,\n",
      "       'recall': 0.9003633581946835,\n",
      "       'f1-score': 0.8715828448009874,\n",
      "       'support': 15687},\n",
      " '1': {'precision': 0.894477450715636,\n",
      "       'recall': 0.8360045431600202,\n",
      "       'f1-score': 0.8642530984996738,\n",
      "       'support': 15848},\n",
      " 'accuracy': 0.8680196606944665,\n",
      " 'macro avg': {'precision': 0.8695313761979783,\n",
      "               'recall': 0.8681839506773519,\n",
      "               'f1-score': 0.8679179716503306,\n",
      "               'support': 31535},\n",
      " 'weighted avg': {'precision': 0.8696587368447943,\n",
      "                  'recall': 0.8680196606944665,\n",
      "                  'f1-score': 0.8678992608662096,\n",
      "                  'support': 31535}}\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1), tokenizer=tknzr.tokenize, strip_accents='unicode', lowercase=True, stop_words=stopwords.words('english'))\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "pipeline.fit(raw_datasets['train']['comment_text'], raw_datasets['train']['label'])\n",
    "\n",
    "\n",
    "preds = pipeline.predict(raw_datasets[\"test\"][\"comment_text\"])\n",
    "\n",
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "pp(metric.compute(predictions=preds, references=raw_datasets[\"test\"][\"label\"]))\n",
    "pp(confusion_matrix(raw_datasets[\"test\"][\"label\"], preds))\n",
    "pp(classification_report(raw_datasets[\"test\"][\"label\"], preds, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67b47a70-2a31-47b5-8fad-e93c77fbab28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/.cache/pypoetry/virtualenvs/mineracao-XH4Rs9qH-py3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1208: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[159348,  17196],\n",
      "       [  2163,  13080]])\n",
      "{'0': {'precision': 0.9866077233129632,\n",
      "       'recall': 0.9025965198477434,\n",
      "       'f1-score': 0.9427341704752186,\n",
      "       'support': 176544},\n",
      " '1': {'precision': 0.43202536662703134,\n",
      "       'recall': 0.8580987994489274,\n",
      "       'f1-score': 0.5747050682132736,\n",
      "       'support': 15243},\n",
      " 'accuracy': 0.8990598945705391,\n",
      " 'macro avg': {'precision': 0.7093165449699973,\n",
      "               'recall': 0.8803476596483354,\n",
      "               'f1-score': 0.758719619344246,\n",
      "               'support': 191787},\n",
      " 'weighted avg': {'precision': 0.9425301848824978,\n",
      "                  'recall': 0.8990598945705391,\n",
      "                  'f1-score': 0.9134836602436657,\n",
      "                  'support': 191787}}\n"
     ]
    }
   ],
   "source": [
    "# Dataset Desbalanceado - Logistic\n",
    "\n",
    "test_df = df[df['split'] == 'test']\n",
    "\n",
    "preds = pipeline.predict(test_df['comment_text'])\n",
    "\n",
    "pp(confusion_matrix(test_df[\"label\"], preds))\n",
    "pp(classification_report(test_df[\"label\"], preds, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06492dd8-a40e-4065-8e54-a29e74e18df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/all_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9316d669-015c-4090-ab13-dc5598075132",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_381762/1087764970.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  bias_df['label'] = np.where(bias_df['toxicity'] >= 0.5, 1, 0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>split</th>\n",
       "      <th>created_date</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>...</th>\n",
       "      <th>asian</th>\n",
       "      <th>latino</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7678</th>\n",
       "      <td>627762</td>\n",
       "      <td>OH yes - Were those evil Christian Missionarie...</td>\n",
       "      <td>train</td>\n",
       "      <td>2016-11-26 15:56:03.862109+00</td>\n",
       "      <td>13</td>\n",
       "      <td>627198.0</td>\n",
       "      <td>152737</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7679</th>\n",
       "      <td>5892815</td>\n",
       "      <td>Why is this black racist crap still on the G&amp;M...</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-09-03 23:20:08.226613+00</td>\n",
       "      <td>54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>373428</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7680</th>\n",
       "      <td>416437</td>\n",
       "      <td>even up here.......BLACKS!</td>\n",
       "      <td>train</td>\n",
       "      <td>2016-08-04 16:48:07.175252+00</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>143025</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7681</th>\n",
       "      <td>5137126</td>\n",
       "      <td>Blame men.  There's always an excuse to blame ...</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-04-15 19:00:45.032674+00</td>\n",
       "      <td>54</td>\n",
       "      <td>5136907.0</td>\n",
       "      <td>327125</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7682</th>\n",
       "      <td>855753</td>\n",
       "      <td>And the woman exposing herself saying grab thi...</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-01-18 01:50:57.478867+00</td>\n",
       "      <td>13</td>\n",
       "      <td>849081.0</td>\n",
       "      <td>162008</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999511</th>\n",
       "      <td>1018736</td>\n",
       "      <td>Another man shamming article. If white men did...</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-02-20 07:20:49.964620+00</td>\n",
       "      <td>54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>169202</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999512</th>\n",
       "      <td>340016</td>\n",
       "      <td>\"no matter what is put in front of you regardi...</td>\n",
       "      <td>train</td>\n",
       "      <td>2016-06-06 06:43:04.780968+00</td>\n",
       "      <td>21</td>\n",
       "      <td>339965.0</td>\n",
       "      <td>137961</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999513</th>\n",
       "      <td>919629</td>\n",
       "      <td>The Democrat party aided and abetted by it's M...</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-01-30 02:44:29.168863+00</td>\n",
       "      <td>54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>164845</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999514</th>\n",
       "      <td>5165492</td>\n",
       "      <td>I just don't find her a very good representati...</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-04-22 18:42:02.442987+00</td>\n",
       "      <td>54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>328877</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003717</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>269</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999515</th>\n",
       "      <td>4984105</td>\n",
       "      <td>You know the Trump fanatics are trolling the G...</td>\n",
       "      <td>train</td>\n",
       "      <td>2017-03-10 00:55:35.369198+00</td>\n",
       "      <td>54</td>\n",
       "      <td>807615.0</td>\n",
       "      <td>156960</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00064</td>\n",
       "      <td>1562</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>442553 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                       comment_text  split  \\\n",
       "7678      627762  OH yes - Were those evil Christian Missionarie...  train   \n",
       "7679     5892815  Why is this black racist crap still on the G&M...  train   \n",
       "7680      416437                         even up here.......BLACKS!  train   \n",
       "7681     5137126  Blame men.  There's always an excuse to blame ...  train   \n",
       "7682      855753  And the woman exposing herself saying grab thi...  train   \n",
       "...          ...                                                ...    ...   \n",
       "1999511  1018736  Another man shamming article. If white men did...  train   \n",
       "1999512   340016  \"no matter what is put in front of you regardi...  train   \n",
       "1999513   919629  The Democrat party aided and abetted by it's M...  train   \n",
       "1999514  5165492  I just don't find her a very good representati...  train   \n",
       "1999515  4984105  You know the Trump fanatics are trolling the G...  train   \n",
       "\n",
       "                          created_date  publication_id  parent_id  article_id  \\\n",
       "7678     2016-11-26 15:56:03.862109+00              13   627198.0      152737   \n",
       "7679     2017-09-03 23:20:08.226613+00              54        NaN      373428   \n",
       "7680     2016-08-04 16:48:07.175252+00              21        NaN      143025   \n",
       "7681     2017-04-15 19:00:45.032674+00              54  5136907.0      327125   \n",
       "7682     2017-01-18 01:50:57.478867+00              13   849081.0      162008   \n",
       "...                                ...             ...        ...         ...   \n",
       "1999511  2017-02-20 07:20:49.964620+00              54        NaN      169202   \n",
       "1999512  2016-06-06 06:43:04.780968+00              21   339965.0      137961   \n",
       "1999513  2017-01-30 02:44:29.168863+00              54        NaN      164845   \n",
       "1999514  2017-04-22 18:42:02.442987+00              54        NaN      328877   \n",
       "1999515  2017-03-10 00:55:35.369198+00              54   807615.0      156960   \n",
       "\n",
       "           rating  funny  wow  ...  asian  latino  other_race_or_ethnicity  \\\n",
       "7678     approved      0    0  ...    0.0     0.0                      0.0   \n",
       "7679     rejected      0    0  ...    0.0     0.0                      0.0   \n",
       "7680     rejected      0    0  ...    0.0     0.0                      0.0   \n",
       "7681     rejected      0    0  ...    0.0     0.0                      0.0   \n",
       "7682     rejected      0    0  ...    0.0     0.0                      0.0   \n",
       "...           ...    ...  ...  ...    ...     ...                      ...   \n",
       "1999511  approved      0    0  ...    0.0     0.0                      0.0   \n",
       "1999512  approved      0    0  ...    0.0     0.0                      0.0   \n",
       "1999513  rejected      0    1  ...    0.0     0.0                      0.0   \n",
       "1999514  approved      1    0  ...    0.0     0.0                      0.0   \n",
       "1999515  approved      1    0  ...    0.0     0.0                      0.0   \n",
       "\n",
       "         physical_disability  intellectual_or_learning_disability  \\\n",
       "7678                0.000000                                  0.0   \n",
       "7679                0.000000                                  0.0   \n",
       "7680                0.000000                                  0.0   \n",
       "7681                0.000000                                  0.0   \n",
       "7682                0.000000                                  0.0   \n",
       "...                      ...                                  ...   \n",
       "1999511             0.000000                                  0.0   \n",
       "1999512             0.000000                                  0.0   \n",
       "1999513             0.000000                                  0.0   \n",
       "1999514             0.003717                                  0.0   \n",
       "1999515             0.000000                                  0.0   \n",
       "\n",
       "         psychiatric_or_mental_illness  other_disability  \\\n",
       "7678                               0.0           0.00000   \n",
       "7679                               0.0           0.00000   \n",
       "7680                               0.0           0.00000   \n",
       "7681                               0.0           0.00000   \n",
       "7682                               0.0           0.00000   \n",
       "...                                ...               ...   \n",
       "1999511                            0.0           0.00000   \n",
       "1999512                            0.0           0.00000   \n",
       "1999513                            0.0           0.00000   \n",
       "1999514                            0.0           0.00000   \n",
       "1999515                            0.0           0.00064   \n",
       "\n",
       "         identity_annotator_count  toxicity_annotator_count  label  \n",
       "7678                            4                        10      1  \n",
       "7679                            4                        70      1  \n",
       "7680                            4                        61      1  \n",
       "7681                            4                        11      1  \n",
       "7682                            4                        70      1  \n",
       "...                           ...                       ...    ...  \n",
       "1999511                        10                        10      0  \n",
       "1999512                        10                        10      0  \n",
       "1999513                        11                        10      0  \n",
       "1999514                       269                        10      0  \n",
       "1999515                      1562                        10      0  \n",
       "\n",
       "[442553 rows x 47 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Análise de Viés\n",
    "\n",
    "df.dropna(subset=['comment_text', 'toxicity'], inplace=True)\n",
    "# df.dropna(axis='columns', inplace=True)\n",
    "# remove duplicados\n",
    "df.drop_duplicates(subset=['comment_text',], keep='first', inplace=True)\n",
    "df.comment_text = df.comment_text.replace('\\xad', '', regex=True)  # remove os caracteres \\xad que aparecem em alguns spams\n",
    "df.isnull().sum()\n",
    "bias_df = df.dropna(subset=['male',])\n",
    "bias_df['label'] = np.where(bias_df['toxicity'] >= 0.5, 1, 0)\n",
    "bias_df\n",
    "# preds = pipeline.predict(test_df['comment_text'])\n",
    "\n",
    "# pp(confusion_matrix(test_df[\"label\"], preds))\n",
    "# pp(classification_report(test_df[\"label\"], preds, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "51254e82-7e53-433a-b6e5-80ca1d57f45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/.cache/pypoetry/virtualenvs/mineracao-XH4Rs9qH-py3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1208: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[2617, 1080],\n",
      "       [  61,  605]])\n",
      "{'0': {'precision': 0.9772218073188947,\n",
      "       'recall': 0.7078712469569921,\n",
      "       'f1-score': 0.8210196078431372,\n",
      "       'support': 3697},\n",
      " '1': {'precision': 0.3590504451038576,\n",
      "       'recall': 0.9084084084084084,\n",
      "       'f1-score': 0.5146746065504042,\n",
      "       'support': 666},\n",
      " 'accuracy': 0.7384826953930782,\n",
      " 'macro avg': {'precision': 0.6681361262113762,\n",
      "               'recall': 0.8081398276827003,\n",
      "               'f1-score': 0.6678471071967707,\n",
      "               'support': 4363},\n",
      " 'weighted avg': {'precision': 0.8828596420117175,\n",
      "                  'recall': 0.7384826953930782,\n",
      "                  'f1-score': 0.774256882456715,\n",
      "                  'support': 4363}}\n"
     ]
    }
   ],
   "source": [
    "# Análise Bias - Male\n",
    "\n",
    "tbias_df = bias_df[bias_df['split'] == 'test']\n",
    "tbias_df = tbias_df[tbias_df['male'] >= 0.5]\n",
    "\n",
    "preds = pipeline.predict(tbias_df['comment_text'])\n",
    "\n",
    "pp(confusion_matrix(tbias_df[\"label\"], preds))\n",
    "pp(classification_report(tbias_df[\"label\"], preds, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be748c76-9858-470b-a3f5-56276ae19f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/.cache/pypoetry/virtualenvs/mineracao-XH4Rs9qH-py3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1208: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[3340, 1109],\n",
      "       [  83,  606]])\n",
      "{'0': {'precision': 0.9757522640958224,\n",
      "       'recall': 0.7507305012362329,\n",
      "       'f1-score': 0.8485772357723577,\n",
      "       'support': 4449},\n",
      " '1': {'precision': 0.3533527696793003,\n",
      "       'recall': 0.8795355587808418,\n",
      "       'f1-score': 0.5041597337770382,\n",
      "       'support': 689},\n",
      " 'accuracy': 0.7680031140521604,\n",
      " 'macro avg': {'precision': 0.6645525168875613,\n",
      "               'recall': 0.8151330300085373,\n",
      "               'f1-score': 0.6763684847746979,\n",
      "               'support': 5138},\n",
      " 'weighted avg': {'precision': 0.8922891944864444,\n",
      "                  'recall': 0.7680031140521604,\n",
      "                  'f1-score': 0.8023912375483844,\n",
      "                  'support': 5138}}\n"
     ]
    }
   ],
   "source": [
    "# Análise Bias - Female\n",
    "\n",
    "tbias_df = bias_df[bias_df['split'] == 'test']\n",
    "tbias_df = tbias_df[tbias_df['female'] >= 0.5]\n",
    "\n",
    "preds = pipeline.predict(tbias_df['comment_text'])\n",
    "\n",
    "pp(confusion_matrix(tbias_df[\"label\"], preds))\n",
    "pp(classification_report(tbias_df[\"label\"], preds, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3c1fe2d4-91e2-40d9-ab0f-80fa390a6bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/.cache/pypoetry/virtualenvs/mineracao-XH4Rs9qH-py3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1208: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[361, 414],\n",
      "       [ 31, 258]])\n",
      "{'0': {'precision': 0.9209183673469388,\n",
      "       'recall': 0.46580645161290324,\n",
      "       'f1-score': 0.6186803770351328,\n",
      "       'support': 775},\n",
      " '1': {'precision': 0.38392857142857145,\n",
      "       'recall': 0.8927335640138409,\n",
      "       'f1-score': 0.5369406867845994,\n",
      "       'support': 289},\n",
      " 'accuracy': 0.581766917293233,\n",
      " 'macro avg': {'precision': 0.6524234693877551,\n",
      "               'recall': 0.679270007813372,\n",
      "               'f1-score': 0.5778105319098661,\n",
      "               'support': 1064},\n",
      " 'weighted avg': {'precision': 0.7750630562375326,\n",
      "                  'recall': 0.581766917293233,\n",
      "                  'f1-score': 0.5964785250779859,\n",
      "                  'support': 1064}}\n"
     ]
    }
   ],
   "source": [
    "# Análise Bias - homosexual gay or lesbian\n",
    "\n",
    "tbias_df = bias_df[bias_df['split'] == 'test']\n",
    "tbias_df = tbias_df[tbias_df['homosexual_gay_or_lesbian'] >= 0.5]\n",
    "\n",
    "preds = pipeline.predict(tbias_df['comment_text'])\n",
    "\n",
    "pp(confusion_matrix(tbias_df[\"label\"], preds))\n",
    "pp(classification_report(tbias_df[\"label\"], preds, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2dc159b4-bc08-4eb8-bc64-92cc5ed067fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/.cache/pypoetry/virtualenvs/mineracao-XH4Rs9qH-py3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1208: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[3168,  616],\n",
      "       [  67,  347]])\n",
      "{'0': {'precision': 0.9792890262751159,\n",
      "       'recall': 0.8372093023255814,\n",
      "       'f1-score': 0.9026926912665622,\n",
      "       'support': 3784},\n",
      " '1': {'precision': 0.3603322949117342,\n",
      "       'recall': 0.8381642512077294,\n",
      "       'f1-score': 0.5039941902687001,\n",
      "       'support': 414},\n",
      " 'accuracy': 0.8373034778465936,\n",
      " 'macro avg': {'precision': 0.6698106605934251,\n",
      "               'recall': 0.8376867767666554,\n",
      "               'f1-score': 0.7033434407676311,\n",
      "               'support': 4198},\n",
      " 'weighted avg': {'precision': 0.9182485101282747,\n",
      "                  'recall': 0.8373034778465936,\n",
      "                  'f1-score': 0.8633736871186073,\n",
      "                  'support': 4198}}\n"
     ]
    }
   ],
   "source": [
    "# Análise Bias - christian\n",
    "\n",
    "tbias_df = bias_df[bias_df['split'] == 'test']\n",
    "tbias_df = tbias_df[tbias_df['christian'] >= 0.5]\n",
    "\n",
    "preds = pipeline.predict(tbias_df['comment_text'])\n",
    "\n",
    "pp(confusion_matrix(tbias_df[\"label\"], preds))\n",
    "pp(classification_report(tbias_df[\"label\"], preds, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f1eadef-08ed-431b-896f-368b7f9ac769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/.cache/pypoetry/virtualenvs/mineracao-XH4Rs9qH-py3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1208: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[454, 240],\n",
      "       [ 15, 122]])\n",
      "{'0': {'precision': 0.9680170575692963,\n",
      "       'recall': 0.654178674351585,\n",
      "       'f1-score': 0.7807394668959589,\n",
      "       'support': 694},\n",
      " '1': {'precision': 0.3370165745856354,\n",
      "       'recall': 0.8905109489051095,\n",
      "       'f1-score': 0.4889779559118237,\n",
      "       'support': 137},\n",
      " 'accuracy': 0.6931407942238267,\n",
      " 'macro avg': {'precision': 0.6525168160774659,\n",
      "               'recall': 0.7723448116283473,\n",
      "               'f1-score': 0.6348587114038913,\n",
      "               'support': 831},\n",
      " 'weighted avg': {'precision': 0.8639893004468397,\n",
      "                  'recall': 0.6931407942238267,\n",
      "                  'f1-score': 0.7326391937252893,\n",
      "                  'support': 831}}\n"
     ]
    }
   ],
   "source": [
    "# Análise Bias - jewish\n",
    "\n",
    "tbias_df = bias_df[bias_df['split'] == 'test']\n",
    "tbias_df = tbias_df[tbias_df['jewish'] >= 0.5]\n",
    "\n",
    "preds = pipeline.predict(tbias_df['comment_text'])\n",
    "\n",
    "pp(confusion_matrix(tbias_df[\"label\"], preds))\n",
    "pp(classification_report(tbias_df[\"label\"], preds, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ef136df-ec59-4d18-8f7e-6ce3ef460856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/.cache/pypoetry/virtualenvs/mineracao-XH4Rs9qH-py3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1208: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[770, 764],\n",
      "       [ 38, 441]])\n",
      "{'0': {'precision': 0.9529702970297029,\n",
      "       'recall': 0.5019556714471969,\n",
      "       'f1-score': 0.6575576430401366,\n",
      "       'support': 1534},\n",
      " '1': {'precision': 0.3659751037344398,\n",
      "       'recall': 0.9206680584551148,\n",
      "       'f1-score': 0.5237529691211401,\n",
      "       'support': 479},\n",
      " 'accuracy': 0.6015896671634376,\n",
      " 'macro avg': {'precision': 0.6594727003820714,\n",
      "               'recall': 0.7113118649511558,\n",
      "               'f1-score': 0.5906553060806383,\n",
      "               'support': 2013},\n",
      " 'weighted avg': {'precision': 0.8132928516305817,\n",
      "                  'recall': 0.6015896671634376,\n",
      "                  'f1-score': 0.6257183788537485,\n",
      "                  'support': 2013}}\n"
     ]
    }
   ],
   "source": [
    "# Análise Bias - muslim\n",
    "\n",
    "tbias_df = bias_df[bias_df['split'] == 'test']\n",
    "tbias_df = tbias_df[tbias_df['muslim'] >= 0.5]\n",
    "\n",
    "preds = pipeline.predict(tbias_df['comment_text'])\n",
    "\n",
    "pp(confusion_matrix(tbias_df[\"label\"], preds))\n",
    "pp(classification_report(tbias_df[\"label\"], preds, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6bcbd1c1-c261-44fe-a9e1-6d3e817037f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/.cache/pypoetry/virtualenvs/mineracao-XH4Rs9qH-py3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1208: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[375, 636],\n",
      "       [ 24, 472]])\n",
      "{'0': {'precision': 0.9398496240601504,\n",
      "       'recall': 0.37091988130563797,\n",
      "       'f1-score': 0.5319148936170213,\n",
      "       'support': 1011},\n",
      " '1': {'precision': 0.4259927797833935,\n",
      "       'recall': 0.9516129032258065,\n",
      "       'f1-score': 0.5885286783042394,\n",
      "       'support': 496},\n",
      " 'accuracy': 0.5620437956204379,\n",
      " 'macro avg': {'precision': 0.682921201921772,\n",
      "               'recall': 0.6612663922657223,\n",
      "               'f1-score': 0.5602217859606303,\n",
      "               'support': 1507},\n",
      " 'weighted avg': {'precision': 0.7707235492351527,\n",
      "                  'recall': 0.5620437956204379,\n",
      "                  'f1-score': 0.5505482295193836,\n",
      "                  'support': 1507}}\n"
     ]
    }
   ],
   "source": [
    "# Análise Bias - black\n",
    "\n",
    "tbias_df = bias_df[bias_df['split'] == 'test']\n",
    "tbias_df = tbias_df[tbias_df['black'] >= 0.5]\n",
    "\n",
    "preds = pipeline.predict(tbias_df['comment_text'])\n",
    "\n",
    "pp(confusion_matrix(tbias_df[\"label\"], preds))\n",
    "pp(classification_report(tbias_df[\"label\"], preds, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dd48a62a-af5b-48a6-92c8-3b4507ed96ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/.cache/pypoetry/virtualenvs/mineracao-XH4Rs9qH-py3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1208: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 685, 1010],\n",
      "       [  28,  705]])\n",
      "{'0': {'precision': 0.9607293127629734,\n",
      "       'recall': 0.40412979351032446,\n",
      "       'f1-score': 0.5689368770764119,\n",
      "       'support': 1695},\n",
      " '1': {'precision': 0.4110787172011662,\n",
      "       'recall': 0.9618008185538881,\n",
      "       'f1-score': 0.5759803921568628,\n",
      "       'support': 733},\n",
      " 'accuracy': 0.5724876441515651,\n",
      " 'macro avg': {'precision': 0.6859040149820698,\n",
      "               'recall': 0.6829653060321063,\n",
      "               'f1-score': 0.5724586346166374,\n",
      "               'support': 2428},\n",
      " 'weighted avg': {'precision': 0.7947927861786223,\n",
      "                  'recall': 0.5724876441515651,\n",
      "                  'f1-score': 0.571063275986614,\n",
      "                  'support': 2428}}\n"
     ]
    }
   ],
   "source": [
    "# Análise Bias - white\n",
    "\n",
    "tbias_df = bias_df[bias_df['split'] == 'test']\n",
    "tbias_df = tbias_df[tbias_df['white'] >= 0.5]\n",
    "\n",
    "preds = pipeline.predict(tbias_df['comment_text'])\n",
    "\n",
    "pp(confusion_matrix(tbias_df[\"label\"], preds))\n",
    "pp(classification_report(tbias_df[\"label\"], preds, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8fbfdc0a-1db3-4275-b880-114d119fbd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[244, 160],\n",
      "       [  5,  99]])\n",
      "{'0': {'precision': 0.9799196787148594,\n",
      "       'recall': 0.6039603960396039,\n",
      "       'f1-score': 0.7473200612557428,\n",
      "       'support': 404},\n",
      " '1': {'precision': 0.38223938223938225,\n",
      "       'recall': 0.9519230769230769,\n",
      "       'f1-score': 0.5454545454545454,\n",
      "       'support': 104},\n",
      " 'accuracy': 0.6751968503937008,\n",
      " 'macro avg': {'precision': 0.6810795304771209,\n",
      "               'recall': 0.7779417364813404,\n",
      "               'f1-score': 0.6463873033551442,\n",
      "               'support': 508},\n",
      " 'weighted avg': {'precision': 0.8575599329797224,\n",
      "                  'recall': 0.6751968503937008,\n",
      "                  'f1-score': 0.7059932627452614,\n",
      "                  'support': 508}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/.cache/pypoetry/virtualenvs/mineracao-XH4Rs9qH-py3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1208: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Análise Bias - psychiatric_or_mental_illness\n",
    "\n",
    "tbias_df = bias_df[bias_df['split'] == 'test']\n",
    "tbias_df = tbias_df[tbias_df['psychiatric_or_mental_illness'] >= 0.5]\n",
    "\n",
    "preds = pipeline.predict(tbias_df['comment_text'])\n",
    "\n",
    "pp(confusion_matrix(tbias_df[\"label\"], preds))\n",
    "pp(classification_report(tbias_df[\"label\"], preds, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fc7b53e-419a-462c-896a-1002eee2ed57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.7907404471222451, 'f1': 0.7781475878298872}\n",
      "array([[13363,  2324],\n",
      "       [ 4275, 11573]])\n",
      "{'0': {'precision': 0.7576255811316476,\n",
      "       'recall': 0.8518518518518519,\n",
      "       'f1-score': 0.801980495123781,\n",
      "       'support': 15687},\n",
      " '1': {'precision': 0.8327696625170901,\n",
      "       'recall': 0.7302498738011105,\n",
      "       'f1-score': 0.7781475878298872,\n",
      "       'support': 15848},\n",
      " 'accuracy': 0.7907404471222451,\n",
      " 'macro avg': {'precision': 0.7951976218243688,\n",
      "               'recall': 0.7910508628264812,\n",
      "               'f1-score': 0.7900640414768341,\n",
      "               'support': 31535},\n",
      " 'weighted avg': {'precision': 0.7953894435637545,\n",
      "                  'recall': 0.7907404471222451,\n",
      "                  'f1-score': 0.7900032027567719,\n",
      "                  'support': 31535}}\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression - remove_non_alpha\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1), tokenizer=tknzr.tokenize, strip_accents='unicode', lowercase=True, stop_words=stopwords.words('english'))\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('remove_non_alpha', FunctionTransformer(remove_non_alpha)),\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "pipeline.fit(raw_datasets['train']['comment_text'], raw_datasets['train']['label'])\n",
    "\n",
    "\n",
    "preds = pipeline.predict(raw_datasets[\"test\"][\"comment_text\"])\n",
    "\n",
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "pp(metric.compute(predictions=preds, references=raw_datasets[\"test\"][\"label\"]))\n",
    "pp(confusion_matrix(raw_datasets[\"test\"][\"label\"], preds))\n",
    "pp(classification_report(raw_datasets[\"test\"][\"label\"], preds, output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "969fc938-b35a-42cc-a654-67f2bbb5c7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafael/.cache/pypoetry/virtualenvs/mineracao-XH4Rs9qH-py3.9/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1208: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8329792294276201, 'f1': 0.8358729861955065}\n",
      "array([[12856,  2831],\n",
      "       [ 2436, 13412]])\n",
      "{'0': {'precision': 0.8407010201412504,\n",
      "       'recall': 0.8195320966405304,\n",
      "       'f1-score': 0.8299816004390071,\n",
      "       'support': 15687},\n",
      " '1': {'precision': 0.8257095364156868,\n",
      "       'recall': 0.8462897526501767,\n",
      "       'f1-score': 0.8358729861955065,\n",
      "       'support': 15848},\n",
      " 'accuracy': 0.8329792294276201,\n",
      " 'macro avg': {'precision': 0.8332052782784686,\n",
      "               'recall': 0.8329109246453535,\n",
      "               'f1-score': 0.8329272933172569,\n",
      "               'support': 31535},\n",
      " 'weighted avg': {'precision': 0.8331670092301126,\n",
      "                  'recall': 0.8329792294276201,\n",
      "                  'f1-score': 0.8329423323707974,\n",
      "                  'support': 31535}}\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression 1-4-gram com Feature Selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,4), tokenizer=tknzr.tokenize, strip_accents='unicode', lowercase=True, stop_words=stopwords.words('english'))\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', vectorizer),\n",
    "    ('kbest', SelectKBest(k=1000)),\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "pipeline.fit(raw_datasets['train']['comment_text'], raw_datasets['train']['label'])\n",
    "\n",
    "\n",
    "preds = pipeline.predict(raw_datasets[\"test\"][\"comment_text\"])\n",
    "\n",
    "metric = load_metric(\"glue\", \"mrpc\")\n",
    "pp(metric.compute(predictions=preds, references=raw_datasets[\"test\"][\"label\"]))\n",
    "pp(confusion_matrix(raw_datasets[\"test\"][\"label\"], preds))\n",
    "pp(classification_report(raw_datasets[\"test\"][\"label\"], preds, output_dict=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
